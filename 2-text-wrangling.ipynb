{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "cnx = mysql.connector.connect(user='10.1.50.128', password='hadoop',\n",
    "                              host='10.1.50.82',\n",
    "                              database='ImsPocData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is an example sent.',\n",
       " 'The sentence splitter will split on sent markers.',\n",
       " 'Ohh really !',\n",
       " '!']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "inputstr = 'This is an example sent. The sentence splitter will split on sent markers. Ohh really !!'\n",
    "sents = sent_tokenize(inputstr)\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'sent',\n",
       " '.',\n",
       " 'The',\n",
       " 'sentence',\n",
       " 'splitter',\n",
       " 'willsplit',\n",
       " 'on',\n",
       " 'sent',\n",
       " 'markers',\n",
       " '.',\n",
       " 'Ohh',\n",
       " 'really',\n",
       " '!!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "word_tokenize(inputstr)\n",
    "regexp_tokenize(inputstr, '\\w+') #by words and digits, no special symbols\n",
    "regexp_tokenize(inputstr, '\\w+') #by numbers only\n",
    "blankline_tokenize(inputstr)\n",
    "wordpunct_tokenize(inputstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "## Cutting down the branches of a tree to its stem. \n",
    "### In NLP it means cutting down any token to its stem.\n",
    "### For instance words like  *eat, eating. eaten* all have the same stem. It can be used to club all grammatical variations of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marri\n",
      "marri\n",
      "marry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()\n",
    "sst = SnowballStemmer(\"english\")\n",
    "\n",
    "print pst.stem(\"married\")\n",
    "print sst.stem(\"married\")\n",
    "print lst.stem(\"married\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "### Lemmatization is a more methodical form of *stemming*. It uses context and part of speech to determine the inflected form of the word and applies different normalization rules for each part of speech to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'eat'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "welm = WordNetLemmatizer()\n",
    "welm.lemmatize(\"ate\", pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal\n",
    "### Remove words that occur most commonly across all the documents in the corpus, typically articles, pronouns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sent', 'sentence', 'splitter', 'split', 'sent', 'markers', 'Ohh', 'really']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "cleanwordlist = [word for word in regexp_tokenize(inputstr, '\\w+') if word.lower() not in stoplist]\n",
    "print cleanwordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare word removal\n",
    "### Words that are very unique in nature like names, brands, product names, noise characters and html leftouts need to be removed as they would make really bad predictors for text classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell checker\n",
    "### Most commonly used spell-checker is edit-distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "print edit_distance(\"rain\", \"shine\")\n",
    "print edit_distance(\"elephant\", \"eelphant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
